Dit, a distributed relational database
======================================


Purposes
========

Dit is distributed for the purposes of:

1. Control

Separate actors have their own independent copies of [nice: copies of subsets
of] a database. This is so they retain their own agency. 

2. Collaboration

Separate actors can efficiently synchronise changes to their copies of the
database [nice: changes to subsets] with each other.

This works whether a human or artificial agent made the changes. It can happen
peerwise in any direction between any pair of agents. Merging data and dealing
with conflicts happens in a robust manner.

3. (Scaling)

For one actor, dit can be used to have multiple copies of the same data [nice:
subsets of the same data] on separate computers, syncing updates. This can help
them effectively scale for large load volumes.


Motivation
==========

If an upstream actor disappears or changes economic or political behaviour,
dit offers independence. 

If an actor is on the edge of the network -- high latency or low bandwidth or
no connectivity to all or part of the rest of the network -- their applications
will continue to work with reduced collaboration.

This happens during a rare network outage, in a remote area on a low bandwidth
satellite link, on an aeroplane in the early C21 (in space later), during an
extreme disaster situation.

This happens when restricted for political reasons to just one continent's
network or just to a very local mesh network.

It's easy if you're sitting in a high bandwidth part of the modern, centralised
Internet to not care about any of the above. While this is risky, in the short
term it will work. However, it misses the real advantage.

The real advantage is an extra abstraction layer that simplifies all sorts of
common things that we do. Sharing data, copying data, submitting changes to
data, getting updates of data - all are currently handled in an adhoc manner,
with often complex code specific to each use case.


Need for a new project
======================

CouchDB and git are the nearest two existing projects that got noticeable
traction. Neither is suitable for our purposes.

Couched in danger
-----------------

CouchDB has failed because of the distraction of the CouchBase rewrite, which
changed it from focussing on all three purposes, to just one.

There was an economic motivation to use Couch's original idea principally for
scaling within one actor -- so called Big Data.

Additionally, the underlying database had its own new query language. The query
language for dit should be relational, and very compatible with SQL. This is
just like git worked with recursive trees of text files.

git is for text
---------------

git is designed for storing recursive trees of text files, which are indexed
when instantiated only as a navigational database - i.e. a filesystem with
paths to look up data.

While git can be adapted to storing other data, it's not fundamentally designed
to efficiently work with relational data. 

It's possible an implementation of dit could be made by, say, storing JSON
files in git, or changing git's backend to work efficiently with SQLite files.
That, however, would be an implementation detail, and likely restricted to an
initial prototype.


Expected use as end user
========================

1. My database, if I've synced it all to a computer, is fully available with
all facilities on that computer.

( 2. My database keeps all history of changes in the natural course 
of events. )

[ 3. I can check out any version of my database fairly quickly, and can
then query that as if it were the latest data. ]

4. A database can be efficiently synced with another database, bringing
it up to date with the latest changes.

5. A database can be trivially forked, either by syncing it to a new
database or by copying its files.

( 6. If syncing back together two once forked database, enough semantics will
have been recorded or will be inferrable to let it "do the sensible thing". )

( 7. I can synchronise just part of a database by SQL or time filter, as I
might not have permissions or storage space to synchronise all of it. )

8. I can query a remote database, which maybe I don't have a full copy of
locally, with essentially the same protocol as a local one.

[ 9. I can query slowly across two databases, local and/or remote, with joins
between them as if they were one. ]

[ 10. My database just uses SQLite or PostgreSQL or similar as its indexing
backend / query engine, so it is compatible with their SQL. ]

11. My database uses a large subset of actual SQL as its query language.
It is made as compatible as possible with other SQL databases.


Notes on uses
-------------

I'm not completely sure we need 7 and 8, git doesn't have them. It has related
features such as making an entirely new repository which is a filter of an old
one, but that loses merge commonality; and of course you can query a remote
database by syncing all of it first...

I think we need 8 as data can be an arbitarily large thing, source code isn't.

I'm not sure though, as you could always use 7 with a clever filter to sync
just the data you're allowed to work with. That doesn't allow analytics across
all of a large database "select avg(call_length) from all_telephone_calls".


Nomenclature of this document
=============================

Critical/Essential - no brackets
Optional () - round brackets
Nice [] - square brackets



Todo
====

Need to write more of the motivation for it that had inside ScraperWiki - lots
of the above has turned into more the redecentralisation motivation.

A database is a directory on a filesystem
Separate backend, so if on one system can share storage between forks of databases.
Indexing is a lazy backend thing done for speed, and with hints.
Or should we think of the "filesystem" as simply being a field of SQL queries?

Our remote querying is like saying: the filesystem access of a checked out copy
in git should be the same protocol as git:// repository syncing! Funky!

Operations - what do we have?
  * Operational transforms
  * Clojure collections

Should we retrospectively infer operations like git does? That would be loads
better. We need to work out if we can or we can't do that - if the kinds of
operations on data are fundamentally different or the same as source code
in that regard.

Merge strategies







